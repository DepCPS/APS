{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # v2.9.1\n",
    "import pandas as pd # v1.5.3\n",
    "import numpy as np # v1.22.3\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "import time\n",
    "import sklearn # v1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(pd.__version__)\n",
    "print(np.__version__)\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all csv files for a given patient\n",
    "def find_filenames(path, name_filter='patientA', suffix=\".csv\"):\n",
    "    fileset = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "                if file.endswith(suffix) and name_filter in file[:len(name_filter)] and file[len(name_filter)] == '_':\n",
    "                        fileresult = os.path.join(root, file)\n",
    "                        fileset.append(fileresult)\n",
    "    \n",
    "    return fileset\n",
    "\n",
    "\n",
    "def split_sequences_new(sequences, n_steps_in=12, n_steps_out=12):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        x_end_ix = i + n_steps_in\n",
    "        y_end_ix = x_end_ix + n_steps_out\n",
    "        # check if we are beyond the dataset\n",
    "        if y_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern (include control actions in y's)\n",
    "        seq_x, seq_y = sequences[i:x_end_ix, :], sequences[x_end_ix:y_end_ix, :]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# determines the bounds used by the semantic loss function\n",
    "def find_bounds(BG, IOB=None, interval=0.95):\n",
    "    lower_bounds = [] # bg, iob, bg', iob', bg'', iob''\n",
    "    upper_bounds = []\n",
    "\n",
    "    l, u = confidence_interval(BG, interval=interval)\n",
    "    lower_bounds.append(l)\n",
    "    upper_bounds.append(u)\n",
    "\n",
    "    if IOB is not None:\n",
    "        l, u = confidence_interval(IOB, interval=interval)\n",
    "        lower_bounds.append(l)\n",
    "        upper_bounds.append(u)\n",
    "\n",
    "    l, u = confidence_interval(np.diff(BG), interval=interval)\n",
    "    lower_bounds.append(l)\n",
    "    upper_bounds.append(u)\n",
    "\n",
    "    if IOB is not None:\n",
    "        l, u = confidence_interval(np.diff(IOB), interval=interval)\n",
    "        lower_bounds.append(l)\n",
    "        upper_bounds.append(u)\n",
    "\n",
    "    l, u = confidence_interval(np.diff(BG, n=2), interval=interval)\n",
    "    lower_bounds.append(l)\n",
    "    upper_bounds.append(u)\n",
    "\n",
    "    if IOB is not None:\n",
    "        l, u = confidence_interval(np.diff(IOB, n=2), interval=interval)\n",
    "        lower_bounds.append(l)\n",
    "        upper_bounds.append(u)\n",
    "\n",
    "    return lower_bounds, upper_bounds\n",
    "\n",
    "def confidence_interval(x, interval=0.95):\n",
    "    # try:\n",
    "    #     gamma_a, gamma_loc, gamma_scale = stats.gamma.fit(x)\n",
    "    #     ci = stats.gamma.interval(interval, gamma_a, gamma_loc, gamma_scale)\n",
    "    # except:\n",
    "    mean = np.mean(x)\n",
    "    std = np.std(x)\n",
    "    l = mean - 1.96*std\n",
    "    u = mean + 1.96*std\n",
    "    ci = (l, u)\n",
    "    return ci\n",
    "\n",
    "def find_outliers(dbg, k):\n",
    "    q1 = np.quantile(dbg, 0.25)\n",
    "    q3 = np.quantile(dbg, 0.75)\n",
    "    IQR = q3 - q1\n",
    "    outliers = (dbg < (q1 - k*IQR)) | (dbg > (q3 + k*IQR))\n",
    "    return np.where(outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in files from filelist\n",
    "# FYI: nosplit refers to the fact that this function doesn't split the data into\n",
    "# sequences of a fixed size; that is done in split_sequences after the data is already read.\n",
    "# I did this so to split the files into train, CV, and test sets more easily.\n",
    "def read_files_nosplit(filelist, features, min_length=24, k=1.5, verbose=True):\n",
    "    feature_cols = features\n",
    "\n",
    "    X = np.array([])\n",
    "    traces = np.array([])\n",
    "    t = 0\n",
    "\n",
    "    for i, file in enumerate(filelist):\n",
    "        df = pd.read_csv(file)\n",
    "        df['Time'] = pd.to_datetime(df['Time'])\n",
    "\n",
    "        if len(df) == 0: # as a byproduct of the way I processed files, at most 1 csv is empty per patient\n",
    "            if verbose:\n",
    "                print(f\"Trace {i} is empty\")\n",
    "            continue\n",
    "        \n",
    "        # normally only drop the first 3 hours, but if a carbohydrate is consumed, drop next 3 hours as well\n",
    "        start = df.loc[0, 'Time']\n",
    "        end = df.loc[0, 'Time'] + datetime.timedelta(hours=3)\n",
    "        to_drop = df[np.all([df[\"Time\"] >= start, df[\"Time\"] < end], axis=0)].index\n",
    "        also_drop = df[np.all([df['Time'].dt.hour > 7, df['Time'].dt.hour < 19], axis=0)].index\n",
    "        to_drop = set(to_drop).union(also_drop)\n",
    "\n",
    "        for i in range(1, len(df)):\n",
    "            if df.loc[i, 'Carbs'] > 0:\n",
    "                start = df.loc[i, 'Time']\n",
    "                end = df.loc[i, 'Time'] + datetime.timedelta(hours=3)\n",
    "                temp = df[np.all([df[\"Time\"] >= start, df[\"Time\"] < end], axis=0)].index\n",
    "                to_drop = to_drop.union(temp)\n",
    "\n",
    "        df.drop(to_drop, inplace=True)\n",
    "\n",
    "        if len(df) < min_length:\n",
    "            if verbose:\n",
    "                print(f'Not enough data in trace {i} away from carbs')\n",
    "            continue\n",
    "\n",
    "        # if (df['CGM_glucose'] < 70).sum() > 0:\n",
    "        #     if verbose:\n",
    "        #         print(f\"Patient entered hypoglycemia in trace {i}, treated with rescue carbs\")\n",
    "        #     continue\n",
    "\n",
    "        # find outliers by the difference in BG from i to i+1\n",
    "        slopes = np.diff(df['CGM_glucose'])\n",
    "        outliers_idx = df.iloc[find_outliers(slopes, k)].index + 1\n",
    "\n",
    "        # split df along continuous segments\n",
    "        segments = []\n",
    "        indices = df.index.to_list()\n",
    "        seg = [indices[0]]\n",
    "\n",
    "        i = 1\n",
    "        while i < len(indices):\n",
    "            if indices[i] == (seg[-1]+1):\n",
    "                if indices[i] in outliers_idx:\n",
    "                    segments.append(seg)\n",
    "                    seg = []\n",
    "                    i += 1\n",
    "                    if i < len(indices):\n",
    "                        seg.append(indices[i])\n",
    "                        i += 1\n",
    "                else:\n",
    "                    seg.append(indices[i])\n",
    "                    i += 1\n",
    "            else:\n",
    "                segments.append(seg)\n",
    "                seg = [indices[i]]\n",
    "                i += 1\n",
    "        segments.append(seg)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{len(segments)} segment(s) in trace {i} with {len(outliers_idx)} outliers\")\n",
    "\n",
    "        for seg in segments:\n",
    "            if len(seg) < min_length:\n",
    "                if verbose:\n",
    "                    print(f\"Not enough data in this segment of trace {i}\")\n",
    "                continue\n",
    "\n",
    "            seg_df = df.loc[seg, :]\n",
    "\n",
    "            time_increments = np.unique(np.diff(seg_df['Time'])/np.timedelta64(1, 's'))\n",
    "\n",
    "            if len(time_increments) > 1 or time_increments[0] != 300:\n",
    "                if verbose:\n",
    "                    print(f'Timesteps not always 5 min apart for this segment of trace {i}')\n",
    "                continue\n",
    "\n",
    "            # time of day in minutes from 12:00 AM, scaled to be [0, 1)\n",
    "            seg_df['time_of_day'] = (60*seg_df['Time'].dt.hour + seg_df['Time'].dt.minute)/1440\n",
    "\n",
    "            trace_num = t*np.ones(shape=(len(seg_df)))\n",
    "            t += 1\n",
    "            seq = np.c_[seg_df.loc[:, feature_cols]]\n",
    "\n",
    "            if seq.shape[0] > 0:\n",
    "                if len(X)>0:\n",
    "                    X=np.r_[X,seq]\n",
    "                    traces=np.r_[traces, trace_num]\n",
    "                else:\n",
    "                    X=seq\n",
    "                    traces = trace_num\n",
    "\n",
    "    return X, traces \n",
    "    \n",
    "# splits the processed data into input and output sequences of fixed lengths\n",
    "# the input data will be fed into the model, and the output data will be used to \n",
    "# judge how accurate the model is\n",
    "def split_sequences(X, traces, steps_in=12, steps_out=12):\n",
    "    X_ = np.array([])\n",
    "    y_ = np.array([])\n",
    "    for i in range(int(traces[-1])):\n",
    "        seq = X[np.where(traces == i)]\n",
    "\n",
    "        if len(seq) > 0:\n",
    "            Xtemp, ytemp = split_sequences_new(seq, n_steps_in=steps_in, n_steps_out=steps_out)\n",
    "\n",
    "            if Xtemp.shape[0] > 0:\n",
    "                if len(X_)>0:\n",
    "                    X_=np.r_[X_,Xtemp]\n",
    "                    y_=np.r_[y_,ytemp]\n",
    "                else:\n",
    "                    X_=Xtemp\n",
    "                    y_=ytemp\n",
    "\n",
    "    return X_, y_\n",
    "\n",
    "# X, y, traces = read_files_nosplit(filelist)\n",
    "\n",
    "# X, y = split_sequences(X, y, traces)\n",
    "# print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process PSO3 data\n",
    "def read_data_pso3(filelist, scaler, features, steps_in=12, steps_out=12, train_set=True, n_states=1, k=1.5, random_state=None):\n",
    "    X, traces = read_files_nosplit(filelist, features, min_length=(steps_in+steps_out), k=k, verbose=False)\n",
    "    if train_set:\n",
    "        print('Processed non-sequenced train data:', X.shape)\n",
    "        X = scaler.fit_transform(X)\n",
    "    else:\n",
    "        print('Processed non-sequenced test data:', X.shape)\n",
    "        X = scaler.transform(X)\n",
    "\n",
    "    if 'IOB' in features and n_states > 1:\n",
    "        L, U = find_bounds(X[:, 0], X[:, 1])\n",
    "    else:\n",
    "        L, U = find_bounds(X[:, 0])\n",
    "\n",
    "    X, y = split_sequences(X, traces, steps_in=steps_in, steps_out=steps_out)\n",
    "    return X, y, L, U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_bounds(L, U, steps_out=12):\n",
    "    L = tf.convert_to_tensor([L], dtype='float32')\n",
    "    U = tf.convert_to_tensor([U], dtype='float32')\n",
    "    L = tf.repeat(L, axis=0, repeats=steps_out)\n",
    "    U = tf.repeat(U, axis=0, repeats=steps_out)\n",
    "    return L, U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, LSTM, Input, TimeDistributed\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def build_model(hidden_dim, n_features, n_states, drop_rate=0.2):\n",
    "    enc_inp = Input(shape=(None, n_features), name='enc_input')\n",
    "    dec_inp = Input(shape=(None, n_features), name='dec_input')\n",
    "\n",
    "    if drop_rate > 0:\n",
    "        drop1 = Dropout(drop_rate)\n",
    "        drop2 = Dropout(drop_rate)\n",
    "\n",
    "    enc = LSTM(hidden_dim, return_state=True, name='encoder')\n",
    "    enc_out, state_h, state_c = enc(enc_inp)\n",
    "    if drop_rate > 0:\n",
    "        state_h, state_c = drop1([state_h, state_c])\n",
    "    enc_state = [state_h, state_c]\n",
    "\n",
    "    dec = LSTM(hidden_dim, return_sequences=True, return_state=True, name='decoder')\n",
    "    dec_out, _, _ = dec(dec_inp, initial_state=enc_state)\n",
    "    if drop_rate > 0:\n",
    "        dec_out = drop2(dec_out)\n",
    "\n",
    "    ffn = TimeDistributed(Dense(n_states, activation='linear'), name='dense')\n",
    "    out = ffn(dec_out)\n",
    "\n",
    "    model = Model(inputs=[enc_inp, dec_inp], outputs=out)\n",
    "\n",
    "    encoder_model = Model(enc_inp, enc_state)\n",
    "\n",
    "    dec_inp_h = Input(shape=(hidden_dim,))\n",
    "    dec_inp_c = Input(shape=(hidden_dim,))\n",
    "    dec_state_inp = [dec_inp_h, dec_inp_c]\n",
    "    dec_out, dec_h, dec_c = dec(dec_inp, initial_state=dec_state_inp)\n",
    "    dec_states = [dec_h, dec_c]\n",
    "    dec_out = ffn(dec_out)\n",
    "\n",
    "    all_dec_inp = [dec_inp] + dec_state_inp # concat operation for python lists: [1] + [2] = [1, 2]\n",
    "    all_dec_out = [dec_out] + dec_states\n",
    "    decoder_model = Model(all_dec_inp, all_dec_out)\n",
    "\n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, Xtrain, ytrain, optimizer, loss_func, epochs, batches, n_states):\n",
    "    train_loss = tf.keras.metrics.Mean()\n",
    "    train_mse = tf.keras.metrics.Mean()\n",
    "    train_batches = tf.data.Dataset.from_tensor_slices((Xtrain, ytrain)).batch(batches)\n",
    "\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(x, dec_inp, y):\n",
    "        # forward\n",
    "        with tf.GradientTape() as tape:\n",
    "            yhat = model([x, dec_inp], training=True)\n",
    "            # loss = semantic_loss_new(y, yhat)\n",
    "            loss = loss_func(y, yhat)\n",
    "        \n",
    "        # backward\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        # metrics\n",
    "        train_loss(loss)\n",
    "        train_mse(loss_func(y, yhat))\n",
    "        return loss\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if epoch == 100:\n",
    "            lr = 3e-5\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        if epoch == 200:\n",
    "            lr = 1e-5\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        start = time.time()\n",
    "        train_loss.reset_states()\n",
    "        train_mse.reset_states()\n",
    "\n",
    "        for batch, (x, y) in enumerate(train_batches):    \n",
    "            # create decoder input\n",
    "            dec_init = tf.expand_dims(x[:, -1], 1) # (batch, 1, 3)\n",
    "            dec_rest = y[:, :-1, :]\n",
    "            dec_inp = tf.concat([dec_init, dec_rest], axis=1)\n",
    "            \n",
    "            y_states = y[:, :, :n_states]\n",
    "            \n",
    "            loss = train_step(x, dec_inp, y_states)\n",
    "\n",
    "            # if batch % 250 == 0:\n",
    "            #     print(f'Batch {batch} Loss {train_loss.result():.4f} ') # MSE {train_mse.result():.4f}\n",
    "        \n",
    "        if epoch == 0 or  (epoch+1)% 10 == 0:\n",
    "            print(f'Epoch {epoch+1} Train Loss {train_loss.result():.4f} Train MSE {train_mse.result():.4f} in {time.time() - start:.3f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prediction(x, y, encoder_model, decoder_model, features, steps_out=24, n_states=1):\n",
    "    n_inputs = len(features) - n_states\n",
    "    states_value = encoder_model(x, training=False)\n",
    "\n",
    "    outputs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    dec_init = x[:, -1:, :]\n",
    "\n",
    "    for i in range(steps_out):\n",
    "        yhat_t, h, c = decoder_model([dec_init] + states_value, training=False)\n",
    "        # yhat2 = model([x, dec_init], training=False)\n",
    "        \n",
    "        dec_init = tf.concat([yhat_t, tf.cast(y[:, i:i+1, -n_inputs:], dtype='float32')], axis=2)\n",
    "        outputs = outputs.write(i, yhat_t)\n",
    "        # print(dec_init, y[:, i, :])\n",
    "        states_value = [h, c]\n",
    "        \n",
    "    yhat = tf.transpose(outputs.stack(), perm=[1, 0, 3, 2])\n",
    "    yhat = tf.reshape(yhat, shape=(-1, steps_out, n_states))\n",
    "    return yhat\n",
    "\n",
    "def untransform(y, yhat, scaler, features, steps_out=12, n_states=1):\n",
    "    n_inputs = len(features) - n_states\n",
    "    y = tf.cast(y, dtype='float32')\n",
    "    ypred = np.array([])\n",
    "    ytrue = np.array([])\n",
    "\n",
    "    for i in range(steps_out):\n",
    "        bg_pred = yhat[:, i, :1]\n",
    "        bg = y[:, i, :1]\n",
    "\n",
    "        fake_inputs = tf.zeros(shape=(y.shape[0], n_inputs)) # don't need time_of_day since it isn't included in the scaler\n",
    "        if 'IOB' in features and n_states > 1:\n",
    "            iob_pred = yhat[:, i, 1:2]\n",
    "            iob = y[:, i, 1:2]\n",
    "\n",
    "            pred = tf.concat([bg_pred, iob_pred, fake_inputs], axis=1)\n",
    "            true = tf.concat([bg, iob, fake_inputs], axis=1)\n",
    "        else:\n",
    "            pred = tf.concat([bg_pred, fake_inputs], axis=1)\n",
    "            true = tf.concat([bg, fake_inputs], axis=1)\n",
    "        \n",
    "        state_pred = tf.expand_dims(scaler.named_transformers_['scaler'].inverse_transform(pred)[:, :n_states], 1)\n",
    "        state = tf.expand_dims(scaler.named_transformers_['scaler'].inverse_transform(true)[:, :n_states], 1)\n",
    "\n",
    "        if len(ypred)>0:\n",
    "            ypred = tf.concat([ypred, state_pred], axis=1)\n",
    "            ytrue = tf.concat([ytrue, state], axis=1)\n",
    "        else:\n",
    "            ypred = state_pred\n",
    "            ytrue = state\n",
    "            \n",
    "    return ytrue, ypred\n",
    "\n",
    "def rmse_across_time(y, yhat):\n",
    "    root_mean_squared_error = tf.keras.metrics.RootMeanSquaredError()\n",
    "    ret = []\n",
    "\n",
    "    for t in range(y.shape[1]):\n",
    "        root_mean_squared_error.reset_states()\n",
    "        bg_t = y[:, t, 0]\n",
    "        bghat_t = yhat[:, t, 0]\n",
    "\n",
    "        rmse_t = root_mean_squared_error(bg_t, bghat_t)\n",
    "        ret.append(rmse_t.numpy())\n",
    "    \n",
    "    return ret\n",
    "\n",
    "def performance_results(y, yhat):\n",
    "    np.set_printoptions(suppress=True)\n",
    "\n",
    "    mean = np.mean(y - yhat, axis=0)\n",
    "    std = np.std(y - yhat, axis=0)\n",
    "\n",
    "    print('Mean and St. Dev. of Model Error over time:')\n",
    "    print(np.c_[mean, std])\n",
    "\n",
    "    rmse = tf.keras.metrics.RootMeanSquaredError()\n",
    "    ret_rmse = rmse(y, yhat).numpy()\n",
    "    print('Overall RMSE:', ret_rmse)\n",
    "\n",
    "    print('RMSE across time:')\n",
    "    print(rmse_across_time(y, yhat))\n",
    "    return ret_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(encoder_model, decoder_model, Xtest, ytest, scaler, features, steps_out=12, n_states=1):\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((Xtest, ytest)).batch(1028)\n",
    "    test_rmse = tf.keras.metrics.Mean()\n",
    "    test_rmse.reset_states()\n",
    "    root_mean_squared_error = tf.keras.metrics.RootMeanSquaredError()\n",
    "\n",
    "    y_true = np.array([]) # ending shape is (n_samples, n_steps, n_states)\n",
    "    y_pred = np.array([])\n",
    "\n",
    "    for x, y in test_ds:\n",
    "        # yhat = attn_eval_prediction(x, y, steps_out=steps_out)\n",
    "        yhat = eval_prediction(x, y, encoder_model, decoder_model, features, steps_out=steps_out, n_states=n_states)\n",
    "        print(yhat.shape)\n",
    "        \n",
    "        y_states, yhat = untransform(y, yhat, scaler, features, steps_out=steps_out, n_states=n_states)\n",
    "        print(y_states.shape, yhat.shape)\n",
    "\n",
    "\n",
    "        rmse = root_mean_squared_error(y_states, yhat)\n",
    "        test_rmse(rmse)\n",
    "        # print('.', end='')\n",
    "\n",
    "        if len(y_true) > 0:\n",
    "            # print(y_true.shape, y_states.shape)\n",
    "            y_true = np.r_[y_true, y_states]\n",
    "            y_pred = np.r_[y_pred, yhat]\n",
    "        else:\n",
    "            y_true = y_states\n",
    "            y_pred = yhat\n",
    "            # print(y_true)\n",
    "\n",
    "    print(y_true.shape, y_pred.shape)\n",
    "    res = test_rmse.result().numpy()\n",
    "    print(\"Test RMSE:\", res)\n",
    "    print('BG Results')\n",
    "    res2 = performance_results(y_true[:, :, :1], y_pred[:, :, :1])\n",
    "    print('IOB Results')\n",
    "    res3 = performance_results(y_true[:, :, -1:], y_pred[:, :, -1:])\n",
    "    return res, res2, res3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_traces(enc, dec, X, y, features, scaler, patient, steps_in=24, steps_out=24, n_states=1):\n",
    "    for _ in range(10):\n",
    "        i = np.random.randint(low=0, high=len(X)-1)\n",
    "        x_ = tf.expand_dims(X[i], 0)\n",
    "        y_ = tf.expand_dims(y[i], 0)\n",
    "\n",
    "        yhat = eval_prediction(x_, y_, enc, dec, features, steps_out, n_states)\n",
    "        y_, yhat = untransform(y_, yhat, scaler, features, steps_out)\n",
    "        x_ = x_[:, :, :2]\n",
    "        x_, _ = untransform(x_, np.zeros_like(x_), scaler, features, steps_in)\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax2 = ax.twinx()\n",
    "\n",
    "        ax.plot(range(steps_in), x_[0, :, 0], 'o--', label='BG input')\n",
    "        ax.plot(range(steps_in, steps_in+steps_out), y_[0, :, 0], 'o--', label='BG true')\n",
    "        ax.plot(range(steps_in, steps_in+steps_out), yhat[0, :, 0], 'o--', label='BG pred')\n",
    "\n",
    "        if 'IOB' in features:\n",
    "            ax2.plot(range(steps_in), x_[0, :, 1], 'ro--', label='IOB input')\n",
    "            ax2.plot(range(steps_in, steps_in+steps_out), y_[0, :, 1], 'ro--', label='IOB true')\n",
    "            ax2.plot(range(steps_in, steps_in+steps_out), yhat[0, :, 1], 'yo--', label='IOB pred')\n",
    "            ax2.legend(loc='lower left')\n",
    "\n",
    "        ax.legend(loc='upper left')\n",
    "\n",
    "        path = './results/plots/'+patient\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        plt.savefig(path+'/trace_'+str(i))\n",
    "        plt.clf()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "path = \"/Users/liuyushen/Desktop/BG Prediction/data/openAPS_patient\"\n",
    "# patient = 'patient'+str(2)\n",
    "features = ['CGM_glucose', 'rate']\n",
    "steps_in = 12\n",
    "steps_out = 6\n",
    "random_state=41\n",
    "scaled = True\n",
    "hidden_dim=64\n",
    "drop_rate=0\n",
    "\n",
    "### training parameters ###\n",
    "EPOCHS = 250\n",
    "BATCHES = 64\n",
    "\n",
    "# scaled: lr = 5e-4, epochs = 250\n",
    "# unscaled: lr = 1e-3, epochs = 200+\n",
    "lr = 1e-4\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "base_loss = tf.keras.losses.MeanSquaredError()\n",
    "omega=50\n",
    "n_states=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is ( true, predicted )\n",
    "def semantic_loss_new(x, xhat):\n",
    "    mse = base_loss(x, xhat)\n",
    "\n",
    "    dxhat = tf.concat((tf.zeros(shape=(xhat.shape[0], 1, n_states)), tf.experimental.numpy.diff(xhat, axis=1)), axis=1) # (batch, steps_in, n_features)\n",
    "    d2xhat = tf.concat((tf.zeros(shape=(xhat.shape[0], 2, n_states)), tf.experimental.numpy.diff(xhat, axis=1, n=2)), axis=1)\n",
    "    Shat = tf.concat([xhat, dxhat, d2xhat], axis=2)\n",
    "\n",
    "    reg = tf.reduce_any(tf.logical_or(tf.less(Shat, L), tf.greater(Shat, U)), axis=2) # rule violated in timestep i?\n",
    "    reg = tf.cast(reg, dtype='float32')\n",
    "\n",
    "    S = tf.minimum(tf.maximum(Shat, L), U)\n",
    "    \n",
    "    dist = tf.norm(S - Shat, ord='euclidean', axis=-1)\n",
    "    sem = omega*tf.reduce_mean(reg*dist)\n",
    "\n",
    "    return mse + sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for patient_idx in range(26, 51):\n",
    "    patient = 'patient'+str(patient_idx)\n",
    "    print(f\"##### Starting {patient} #####\")\n",
    "    all_rmse = []\n",
    "    bg_rmse = []\n",
    "    iob_rmse = []\n",
    "\n",
    "    filelist = np.array(find_filenames(path, patient))\n",
    "    if len(filelist) == 0:\n",
    "        print(f'No data for {patient} \\n')\n",
    "        continue\n",
    "\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    for cv, (train_idx, test_idx) in enumerate(kfold.split(filelist)):\n",
    "        print(f\"--- CV {cv+1} ---\")\n",
    "        filelist_train = filelist[train_idx]\n",
    "        filelist_test = filelist[test_idx]\n",
    "\n",
    "        if 'time_of_day' in features:\n",
    "            scaler = ColumnTransformer([('scaler', StandardScaler(), [i for i in range(len(features)-1)])], remainder='passthrough') # doesn't scale time of day\n",
    "        else:\n",
    "            scaler = ColumnTransformer([('scaler', StandardScaler(), [i for i in range(len(features))])], remainder='passthrough')\n",
    "        Xtrain, ytrain, L, U = read_data_pso3(filelist_train, scaler, features, steps_in, steps_out, train_set=True, n_states=n_states, random_state=random_state)\n",
    "        L, U = shape_bounds(L, U, steps_out)\n",
    "        Xtest, ytest, _, _ = read_data_pso3(filelist_test, scaler, features, steps_in, steps_out, train_set=False, random_state=random_state)\n",
    "        print(Xtrain.shape, Xtest.shape)\n",
    "        print(ytrain.shape, ytest.shape)\n",
    "\n",
    "        if len(Xtrain) <= 0 or len(Xtest) <= 0:\n",
    "            continue\n",
    "\n",
    "        model, encoder_model, decoder_model = build_model(hidden_dim, len(features), n_states, drop_rate=drop_rate)\n",
    "\n",
    "        print('Begin training...')\n",
    "        train_model(model, Xtrain, ytrain, optimizer, base_loss, EPOCHS, BATCHES, n_states)\n",
    "        print('Begin evaluation...')\n",
    "        cv_rmse_, bg_rmse_, iob_rmse_ = evaluate_model(encoder_model, decoder_model, Xtest, ytest, scaler, features, steps_out, n_states)\n",
    "        all_rmse.append(cv_rmse_)\n",
    "        bg_rmse.append(bg_rmse_)\n",
    "        iob_rmse.append(iob_rmse_)\n",
    "        \n",
    "        print('\\n')\n",
    "    \n",
    "    # graph_traces(encoder_model, decoder_model, Xtest, ytest, features, scaler, patient, steps_in, steps_out, n_states)\n",
    "\n",
    "    print(f'{patient} average RMSE across {cv+1} CV sets:', np.mean(all_rmse), '(BG:', np.mean(bg_rmse), ', IOB:', np.mean(iob_rmse), ')')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_ode_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
